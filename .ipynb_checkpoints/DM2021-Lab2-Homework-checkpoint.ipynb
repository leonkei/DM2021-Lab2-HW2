{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: æž—æ¾¤åŸº\n",
    "\n",
    "Student ID: 110062558\n",
    "\n",
    "GitHub ID: leonkei\n",
    "\n",
    "Kaggle name: lckleon\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "[Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2021-Lab2-master Repo](https://github.com/fhcalderon87/DM2021-Lab2-master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/c/dm2021-lab2-hw2/) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n",
    "    Submit your last submission __BEFORE the deadline (Dec. 24th 11:59 pm, Friday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 29th 11:59 pm, Wednesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:01:38.017634Z",
     "start_time": "2021-12-22T08:01:38.006631Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:01:40.447188Z",
     "start_time": "2021-12-22T08:01:40.442187Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_progress(i,n):\n",
    "    print('\\r' + '[Progress]:[%s%s]%.2f%%;' % (\n",
    "        'â–ˆ' * int((i+1)*20/n), ' ' * (20-int((i+1)*20/n)),\n",
    "        float((i+1)/n*100)), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:01:42.907771Z",
     "start_time": "2021-12-22T08:01:41.117175Z"
    }
   },
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "#train_labels = []\n",
    "train_ids = []\n",
    "#test_labels = []\n",
    "test_ids = []\n",
    "with open('./dm2021-lab2-hw2/data_identification.csv', 'r') as csv_file:\n",
    "    csv_reader = reader(csv_file)\n",
    "    list_of_rows = list(csv_reader)\n",
    "    for i in range(1, len(list_of_rows)):\n",
    "        if(list_of_rows[i][1]=='train'):\n",
    "            train_ids.append(list_of_rows[i][0])\n",
    "        else:\n",
    "            test_ids.append(list_of_rows[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:02:38.247900Z",
     "start_time": "2021-12-22T08:02:36.691549Z"
    }
   },
   "outputs": [],
   "source": [
    "emo_dict = {}\n",
    "with open('./dm2021-lab2-hw2/emotion.csv', 'r') as csv_file:\n",
    "    csv_reader = reader(csv_file)\n",
    "    list_of_rows = list(csv_reader)\n",
    "    for i in range(1, len(list_of_rows)):\n",
    "        emo_dict[list_of_rows[i][0]] = list_of_rows[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:06:36.022207Z",
     "start_time": "2021-12-22T08:05:28.233104Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "      <th>_source.tweet.hashtags</th>\n",
       "      <th>_source.tweet.tweet_id</th>\n",
       "      <th>_source.tweet.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>376</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>989</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>827</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2015-05-12 12:51:52</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>368</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2017-10-02 17:54:04</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>498</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-10-10 11:04:32</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>840</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-09-02 14:25:06</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>360</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-11-16 01:40:07</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         _score          _index           _crawldate   _type  \\\n",
       "0           391  hashtag_tweets  2015-05-23 11:42:47  tweets   \n",
       "1           433  hashtag_tweets  2016-01-28 04:52:09  tweets   \n",
       "2           232  hashtag_tweets  2017-12-25 04:39:20  tweets   \n",
       "3           376  hashtag_tweets  2016-01-24 23:53:05  tweets   \n",
       "4           989  hashtag_tweets  2016-01-08 17:18:59  tweets   \n",
       "...         ...             ...                  ...     ...   \n",
       "1867530     827  hashtag_tweets  2015-05-12 12:51:52  tweets   \n",
       "1867531     368  hashtag_tweets  2017-10-02 17:54:04  tweets   \n",
       "1867532     498  hashtag_tweets  2016-10-10 11:04:32  tweets   \n",
       "1867533     840  hashtag_tweets  2016-09-02 14:25:06  tweets   \n",
       "1867534     360  hashtag_tweets  2016-11-16 01:40:07  tweets   \n",
       "\n",
       "                  _source.tweet.hashtags _source.tweet.tweet_id  \\\n",
       "0                             [Snapchat]               0x376b20   \n",
       "1          [freepress, TrumpLegacy, CNN]               0x2d5350   \n",
       "2                           [bibleverse]               0x28b412   \n",
       "3                                     []               0x1cd5b0   \n",
       "4                                     []               0x2de201   \n",
       "...                                  ...                    ...   \n",
       "1867530  [mixedfeeling, butimTHATperson]               0x316b80   \n",
       "1867531                               []               0x29d0cb   \n",
       "1867532                               []               0x2a6a4f   \n",
       "1867533                               []               0x24faed   \n",
       "1867534                    [Sundayvibes]               0x34be8c   \n",
       "\n",
       "                                        _source.tweet.text  \n",
       "0        People who post \"add me on #Snapchat\" must be ...  \n",
       "1        @brianklaas As we see, Trump is dangerous to #...  \n",
       "2        Confident of your obedience, I write to you, k...  \n",
       "3                      Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>  \n",
       "4        \"Trust is not the same as faith. A friend is s...  \n",
       "...                                                    ...  \n",
       "1867530  When you buy the last 2 tickets remaining for ...  \n",
       "1867531  I swear all this hard work gone pay off one da...  \n",
       "1867532  @Parcel2Go no card left when I wasn't in so I ...  \n",
       "1867533  Ah, corporate life, where you can date <LH> us...  \n",
       "1867534             Blessed to be living #Sundayvibes <LH>  \n",
       "\n",
       "[1867535 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "file = open('./dm2021-lab2-hw2/tweets_DM.json','r',encoding='utf-8')\n",
    "tweets = []\n",
    "count=0\n",
    "for line in file.readlines():\n",
    "    dic = json.loads(line)\n",
    "    tweets.append(dic)\n",
    "raw_df = pd.DataFrame.from_dict(pd.json_normalize(tweets), orient='columns')\n",
    "raw_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:08:41.333701Z",
     "start_time": "2021-12-22T08:08:41.243678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_source.tweet.tweet_id</th>\n",
       "      <th>_source.tweet.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        _source.tweet.tweet_id  \\\n",
       "0                     0x376b20   \n",
       "1                     0x2d5350   \n",
       "2                     0x28b412   \n",
       "3                     0x1cd5b0   \n",
       "4                     0x2de201   \n",
       "...                        ...   \n",
       "1867530               0x316b80   \n",
       "1867531               0x29d0cb   \n",
       "1867532               0x2a6a4f   \n",
       "1867533               0x24faed   \n",
       "1867534               0x34be8c   \n",
       "\n",
       "                                        _source.tweet.text  \n",
       "0        People who post \"add me on #Snapchat\" must be ...  \n",
       "1        @brianklaas As we see, Trump is dangerous to #...  \n",
       "2        Confident of your obedience, I write to you, k...  \n",
       "3                      Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>  \n",
       "4        \"Trust is not the same as faith. A friend is s...  \n",
       "...                                                    ...  \n",
       "1867530  When you buy the last 2 tickets remaining for ...  \n",
       "1867531  I swear all this hard work gone pay off one da...  \n",
       "1867532  @Parcel2Go no card left when I wasn't in so I ...  \n",
       "1867533  Ah, corporate life, where you can date <LH> us...  \n",
       "1867534             Blessed to be living #Sundayvibes <LH>  \n",
       "\n",
       "[1867535 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label_tweet(i):\n",
    "    return emo_dict[i[\"_source.tweet.tweet_id\"]]\n",
    "\n",
    "df = raw_df[[\"_source.tweet.tweet_id\",\"_source.tweet.text\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T07:15:19.903499Z",
     "start_time": "2021-12-22T07:15:19.891495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People who post \"add me on #Snapchat\" must be dehydrated. Cuz man.... that\\'s <LH>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.iloc[0][\"_source.tweet.text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:32:51.361413Z",
     "start_time": "2021-12-22T08:32:51.354411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>\n",
      "Now ISSA is stalking Tasha <LH>\n",
      "Now ISSA is stalking Tasha $EMOJI$$EMOJI$$EMOJI$ <LH>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'now issa is stalking tasha lh  '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import preprocessor as tp\n",
    "# import re\n",
    "# tsample = df.iloc[3][\"_source.tweet.text\"]\n",
    "\n",
    "# print(tsample)\n",
    "# print(tp.clean(tsample))\n",
    "# print(tp.tokenize(tsample))\n",
    "# r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "# emoticons = re.findall(r, tsample)\n",
    "# text = re.sub(r, '', tsample)\n",
    "# text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:37:51.241958Z",
     "start_time": "2021-12-22T09:37:51.238957Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_tweet(i):\n",
    "    return emo_dict[i[\"_source.tweet.tweet_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:38:24.535366Z",
     "start_time": "2021-12-22T09:38:24.520361Z"
    }
   },
   "outputs": [],
   "source": [
    "import preprocessor as tp\n",
    "def text_tweet(i):\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    tsample = i['_source.tweet.text']\n",
    "    tsample = tp.clean(tsample)\n",
    "    emoticons = re.findall(r, tsample)\n",
    "    text = re.sub(r, '', tsample)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text.replace(\" lh\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:41:00.320810Z",
     "start_time": "2021-12-22T09:38:57.972375Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\leon\\anaconda3\\envs\\conda-env\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "c:\\users\\leon\\anaconda3\\envs\\conda-env\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_source.tweet.text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people who post add me on must be dehydrated c...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as we see trump is dangerous to around the wor...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>now issa is stalking tasha</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>thx for the best time tonight what stories hea...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>still waiting on those supplies liscus</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867526</th>\n",
       "      <td>i m so happy the name of this show</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867527</th>\n",
       "      <td>in every circumtance i d like to be thankful t...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867528</th>\n",
       "      <td>there s currently two girls walking around the...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>ah corporate life where you can date using jus...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>blessed to be living</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        _source.tweet.text         Label\n",
       "0        people who post add me on must be dehydrated c...  anticipation\n",
       "1        as we see trump is dangerous to around the wor...       sadness\n",
       "3                             now issa is stalking tasha            fear\n",
       "5        thx for the best time tonight what stories hea...           joy\n",
       "6                 still waiting on those supplies liscus    anticipation\n",
       "...                                                    ...           ...\n",
       "1867526               i m so happy the name of this show             joy\n",
       "1867527  in every circumtance i d like to be thankful t...           joy\n",
       "1867528  there s currently two girls walking around the...           joy\n",
       "1867533  ah corporate life where you can date using jus...           joy\n",
       "1867534                             blessed to be living             joy\n",
       "\n",
       "[1455563 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df.loc[df[\"_source.tweet.tweet_id\"].isin(train_ids)]\n",
    "train_df['Label'] = train_df.apply(lambda i: label_tweet(i),axis=1)\n",
    "train_df['_source.tweet.text'] = train_df.apply(lambda i: text_tweet(i),axis=1)\n",
    "train_df = train_df.drop(\"_source.tweet.tweet_id\",axis=1)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:42:09.251389Z",
     "start_time": "2021-12-22T09:41:33.514618Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\leon\\anaconda3\\envs\\conda-env\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_source.tweet.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confident of your obedience i write to you kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trust is not the same as faith a friend is so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>when do you have enough when are you satisfied...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>god woke you up now chase the day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>in these tough times who do you turn to as you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867525</th>\n",
       "      <td>for this is the message that ye heard from th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867529</th>\n",
       "      <td>there is a lad here which hath five barley lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>when you buy the last tickets remaining for a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>i swear all this hard work gone pay off one day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>no card left when i wasn t in so i have no ide...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        _source.tweet.text\n",
       "2        confident of your obedience i write to you kno...\n",
       "4         trust is not the same as faith a friend is so...\n",
       "9        when do you have enough when are you satisfied...\n",
       "30                     god woke you up now chase the day  \n",
       "33       in these tough times who do you turn to as you...\n",
       "...                                                    ...\n",
       "1867525   for this is the message that ye heard from th...\n",
       "1867529   there is a lad here which hath five barley lo...\n",
       "1867530  when you buy the last tickets remaining for a ...\n",
       "1867531  i swear all this hard work gone pay off one day  \n",
       "1867532  no card left when i wasn t in so i have no ide...\n",
       "\n",
       "[411972 rows x 1 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = df.loc[df[\"_source.tweet.tweet_id\"].isin(test_ids)]\n",
    "test_df['_source.tweet.text'] = test_df.apply(lambda i: text_tweet(i),axis=1)\n",
    "test_df = test_df.drop(\"_source.tweet.tweet_id\",axis=1)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:44:05.697807Z",
     "start_time": "2021-12-22T09:44:04.434524Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.to_pickle(\"train_df.pkl\") \n",
    "test_df.to_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## load a pickle file\n",
    "train_df = pd.read_pickle(\"train_df.pkl\")\n",
    "test_df = pd.read_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T10:22:00.929644Z",
     "start_time": "2021-12-22T10:22:00.615448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anticipation' 'sadness' 'fear' 'joy' 'anger' 'trust' 'disgust'\n",
      " 'surprise']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'anticipation': 0,\n",
       " 'sadness': 1,\n",
       " 'fear': 2,\n",
       " 'joy': 3,\n",
       " 'anger': 4,\n",
       " 'trust': 5,\n",
       " 'disgust': 6,\n",
       " 'surprise': 7}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = train_df['Label'].unique()\n",
    "print(classes)\n",
    "classes_to_index = dict((c, i) for i, c in enumerate(classes))\n",
    "classes_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T14:48:04.322159Z",
     "start_time": "2021-12-22T14:47:51.458258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[61, 58, 519, 983, 18, 14, 263, 19, 1, 1067, 134, 15, 17], [54, 30, 78, 265, 9, 2488, 3, 251, 2, 126, 33, 6], [59, 529, 9, 5656, 1978], [1447, 8, 2, 103, 62, 157, 33, 1347, 1, 55], [91, 318, 14, 165, 5044, 1], [34, 556, 48, 3324], [69, 5462, 20, 116, 2450, 101, 1522, 1581, 58, 4100, 1, 5462, 1267], [2, 2997, 6, 1, 2032, 493, 3, 4878, 2, 2510, 7866, 47, 111, 1, 566, 1002, 6, 39], [4, 34, 1695, 4, 34, 35, 1, 187, 180, 3, 209, 4, 34, 35, 12, 3181, 128, 15, 52, 124, 186, 27, 52, 31], [31, 147, 241, 12, 85, 12, 4360, 6552, 90, 3, 2, 210, 577, 15, 4, 361, 1174, 465]]\n",
      "[[  61   58  519  983   18   14  263   19    1 1067  134   15   17    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]\n",
      " [  54   30   78  265    9 2488    3  251    2  126   33    6    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]\n",
      " [  59  529    9 5656 1978    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]\n",
      " [1447    8    2  103   62  157   33 1347    1   55    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]\n",
      " [  91  318   14  165 5044    1    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]\n",
      " [  34  556   48 3324    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]\n",
      " [  69 5462   20  116 2450  101 1522 1581   58 4100    1 5462 1267    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]\n",
      " [   2 2997    6    1 2032  493    3 4878    2 2510 7866   47  111    1\n",
      "   566 1002    6   39    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]\n",
      " [   4   34 1695    4   34   35    1  187  180    3  209    4   34   35\n",
      "    12 3181  128   15   52  124  186   27   52   31    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]\n",
      " [  31  147  241   12   85   12 4360 6552   90    3    2  210  577   15\n",
      "     4  361 1174  465    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(train_df[\"_source.tweet.text\"])\n",
    "token_seq = tokenizer.texts_to_sequences(train_df.head(10)[\"_source.tweet.text\"])\n",
    "print(token_seq)\n",
    "padded_sequences = pad_sequences(token_seq, truncating='post', maxlen=50, padding='post')\n",
    "print(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T16:11:39.099908Z",
     "start_time": "2021-12-22T16:11:38.774832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100484      you can never have too many friends at a braai  \n",
       "1087006                       feeling baking stars cookies  \n",
       "898612     i hate brandon walsh so much i get angry at cu...\n",
       "99940      some men on the timeline can only get twitter ...\n",
       "232251                      a lady paid for our meal today  \n",
       "                                 ...                        \n",
       "332048     and above all these things put on charity whic...\n",
       "1814818    still a devastating poll for trump in layman s...\n",
       "168809                                          ohhh tears  \n",
       "861169     sponsor shirts are completed and spirit gear h...\n",
       "156019     their behavior is due to a lack of love we nee...\n",
       "Name: _source.tweet.text, Length: 1411896, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_seq = tokenizer.texts_to_sequences(train_df[\"_source.tweet.text\"])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "     train_df[\"_source.tweet.text\"], train_df[\"Label\"], test_size=0.03, random_state=42)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T16:12:32.207882Z",
     "start_time": "2021-12-22T16:12:17.063058Z"
    }
   },
   "outputs": [],
   "source": [
    "train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "padded_train_seq = pad_sequences(train_seq, truncating='post', maxlen=50, padding='post')\n",
    "val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "padded_val_seq = pad_sequences(val_seq, truncating='post', maxlen=50, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T16:13:13.522277Z",
     "start_time": "2021-12-22T16:13:13.168195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 0, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(classes)\n",
    "train_labels  = le.transform(y_train)\n",
    "val_labels  = le.transform(y_val)\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T16:13:58.029404Z",
     "start_time": "2021-12-22T16:13:57.553297Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 16)            160000    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 50, 40)            5920      \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 40)                9760      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 246       \n",
      "=================================================================\n",
      "Total params: 175,926\n",
      "Trainable params: 175,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 16, input_length=50),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),\n",
    "    tf.keras.layers.Dense(8, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T16:53:25.030862Z",
     "start_time": "2021-12-22T16:14:36.212434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "44122/44122 [==============================] - 822s 19ms/step - loss: nan - accuracy: 0.0274 - val_loss: nan - val_accuracy: 0.0271\n",
      "Epoch 2/50\n",
      "44122/44122 [==============================] - 757s 17ms/step - loss: nan - accuracy: 0.0274 - val_loss: nan - val_accuracy: 0.0271\n",
      "Epoch 3/50\n",
      "44122/44122 [==============================] - 751s 17ms/step - loss: nan - accuracy: 0.0274 - val_loss: nan - val_accuracy: 0.0271\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(\n",
    "    padded_train_seq, train_labels,\n",
    "    validation_data=(padded_val_seq, val_labels),\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T01:56:34.750340Z",
     "start_time": "2021-12-23T01:56:34.744338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1411896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  12, 1597,  134,    9,  270, 3941,   77,  621,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(padded_train_seq))\n",
    "padded_train_seq[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T01:57:14.164882Z",
     "start_time": "2021-12-23T01:57:14.158881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([314, 429, 150,   7, 150, 144, 155,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(padded_val_seq))\n",
    "padded_val_seq[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T15:50:20.727645Z",
     "start_time": "2021-12-22T15:50:20.712642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 6, 2, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T15:50:57.011455Z",
     "start_time": "2021-12-22T15:50:56.996452Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 4, 4, ..., 4, 7, 4])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
